# ğŸ›¡ï¸ ZeroWall â€” AI Moving Target Defense on NVIDIA DGX Spark

> **NVIDIA DGX Spark Hackathon Submission**
> Autonomous, GPU-accelerated AI security that detects attacks, generates hardened code variants, and deploys winning candidates â€” all locally on DGX Spark.

---

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Problem Statement](#problem-statement)
- [Architecture](#architecture)
- [NVIDIA Stack Usage](#nvidia-stack)
- [NVIDIA Requirement Mapping](#requirement-mapping)
- [Demo Instructions](#demo)
- [Benchmark Evidence](#benchmark)
- [Screenshot Checklist](#screenshots)
- [Disclaimer](#disclaimer)

---

## Overview

ZeroWall is a **multi-agent AI Moving Target Defense (MTD) system** that runs entirely on NVIDIA DGX Spark.

When an attack is detected, ZeroWall:
1. Generates **8â€“20 behavior-preserving code mutation candidates** via a safe deterministic transformer
2. **Replays known exploits** against each candidate in parallel
3. **Runs test suites** to verify functional correctness is preserved
4. **Scores risk** using a weighted confidence model (served via Triton)
5. **Deploys the winning variant** and rolls back if post-deploy checks fail

The attack surface keeps moving. Attackers can't re-use the same exploit.

---

## Problem Statement

Static defenses fail against adaptive attackers. Patch cycles are slow. ZeroWall makes the defenders' codebase a **moving target** â€” automatically, continuously, and safely.

---

## Architecture

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚              NVIDIA DGX Spark                    â”‚
                         â”‚                                                  â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ Attacker â”‚â”€â”€exploitâ”€â”€â–º  â”‚         ZeroWall Core Engine                â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚                                             â”‚ â”‚
                         â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚  â”‚    Defense Loop Orchestrator         â”‚   â”‚ â”‚
  â”‚ OpenClaw CLI â”‚â”€â”€â”€â”€â”€â”€â–º  â”‚  â”‚  â”‚  (defense_loop.py)                 â”‚   â”‚ â”‚
  â”‚ /defend      â”‚        â”‚  â”‚  â”‚  1. Mutation Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–ºâ”‚ â”‚
  â”‚ /replay      â”‚        â”‚  â”‚  â”‚  2. Safe Transform Engine (libcst)  â”‚   â”‚ â”‚
  â”‚ /status      â”‚        â”‚  â”‚  â”‚  3. Verifier Agent (pytest+bandit)  â”‚   â”‚ â”‚
  â”‚ /benchmark   â”‚        â”‚  â”‚  â”‚  4. Exploit Agent (HTTP replay)     â”‚   â”‚ â”‚
  â”‚ /alert       â”‚        â”‚  â”‚  â”‚  5. Risk Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–ºâ”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚  â”‚  6. Explanation Agent               â”‚   â”‚ â”‚
                         â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
                         â”‚  â”‚                                             â”‚ â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
  â”‚  Streamlit   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”œâ”€â”€â”¤  â”‚    Triton    â”‚  â”‚  vLLM Server      â”‚  â”‚ â”‚
  â”‚  Dashboard   â”‚        â”‚  â”‚  â”‚  Inference   â”‚  â”‚ (local LLM         â”‚  â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚  â”‚  Server      â”‚  â”‚  reasoning)        â”‚  â”‚ â”‚
                         â”‚  â”‚  â”‚  â€¢ mutation- â”‚  â”‚                   â”‚  â”‚ â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚  â”‚    planner   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
  â”‚    RAPIDS    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”œâ”€â”€â”¤  â”‚  â€¢ risk-     â”‚                         â”‚ â”‚
  â”‚  Analytics   â”‚        â”‚  â”‚  â”‚    scorer    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
  â”‚  (cuDF GPU)  â”‚        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Deploy Controllerâ”‚  â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚                    â”‚  (blue/green swap) â”‚  â”‚ â”‚
                         â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                         â”‚                                                â”‚
                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                         â”‚  â”‚    Target FastAPI App (attack surface)   â”‚  â”‚
                         â”‚  â”‚    v1: vulnerable â†’ vN: hardened         â”‚  â”‚
                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## NVIDIA Stack Usage <a name="nvidia-stack"></a>

### 1. ğŸ–¥ï¸ Triton Inference Server
**What we use it for:** Serving two GPU-accelerated model endpoints:
- `mutation-planner` â€” selects transform types for each defense cycle
- `risk-scorer` â€” scores candidate confidence with batched GPU inference

**Evidence in code:**
- `inference/triton-model-repo/mutation-planner/config.pbtxt` + `1/model.py`
- `inference/triton-model-repo/risk-scorer/config.pbtxt` + `1/model.py`
- `inference/clients/triton_client.py` â€” all agent calls route through Triton HTTP API
- Triton inference latency logged in telemetry and displayed on dashboard

**Docker service:** `docker-compose.yml` â†’ service `triton`

### 2. âš¡ vLLM (Local LLM Runtime â€” TRT-LLM upgrade path)
**What we use it for:** Local GPU LLM inference for:
- Mutation Agent narrative reasoning
- Explanation Agent judge-facing summaries

**Why vLLM and not TRT-LLM directly:** vLLM ships with an OpenAI-compatible API that integrates in minutes. The system is designed with a thin client layer (`inference/clients/vllm_client.py`) so swapping to TRT-LLM requires only pointing the base URL at a TRT-LLM server â€” both expose the same API. On a production DGX Spark with available time, TRT-LLM provides higher token/s throughput.

**Evidence in code:**
- `inference/clients/vllm_client.py`
- `core/agents/explanation_agent.py` â€” calls vLLM for generation
- Per-call latency logged and shown in benchmark output

**Docker service:** `docker-compose.yml` â†’ service `vllm`

### 3. ğŸŒŠ RAPIDS cuDF
**What we use it for:** GPU-accelerated telemetry analytics:
- Exploit success rate before vs after defense cycles
- Defense cycle latency statistics (mean, p95)
- Candidate evaluation counts
- Rolling exploit rate trends for dashboard
- Inference latency breakdown per agent

**Evidence in code:**
- `core/telemetry/rapids_analytics.py` â€” primary cuDF path with pandas fallback
- Uses `cudf.DataFrame` for all analytics operations on DGX Spark
- Backend shown in dashboard ("cuDF-GPU" vs "pandas-CPU")
- Analytics output feeds real-time Streamlit charts

---

## NVIDIA Requirement Mapping <a name="requirement-mapping"></a>

### Why this is NOT laptop-friendly
| Reason | Detail |
|--------|--------|
| Triton GPU models | Requires `KIND_GPU` instance groups â€” CUDA mandatory |
| vLLM LLM inference | Requires GPU; float16 models won't fit in CPU RAM |
| RAPIDS cuDF | CUDA 12+ required; no CPU fallback for real cuDF |
| Parallel agent execution | 8â€“20 candidates + parallel exploit replay saturates GPU |
| Benchmark mode | 50+ concurrent HTTP requests + defense cycle timing requires NVLINK bandwidth |

### NVIDIA Components Used
| Component | Role | Evidence |
|-----------|------|----------|
| Triton Inference Server | Multi-model serving | `inference/triton-model-repo/` |
| vLLM | Local LLM inference (TRT-LLM path) | `inference/clients/vllm_client.py` |
| RAPIDS cuDF | GPU DataFrame analytics | `core/telemetry/rapids_analytics.py` |

### Why this is an Advanced AI System
- **Multi-agent pipeline**: 5 specialized agents (Mutation, Exploit, Verifier, Risk, Explanation)
- **Not a chatbot**: No human in the loop during defense cycle
- **Autonomous decision-making**: Risk Agent recommends deploy/reject/rollback
- **Safe code generation**: Deterministic AST transforms controlled by AI model output
- **Continuous adaptation**: Each cycle produces a different hardened variant

---

## Demo Instructions <a name="demo"></a>

### Prerequisites
```bash
# Clone and setup
cp .env.example .env
# Edit .env: set HF_TOKEN, VLLM_MODEL, GPU counts

# Install local deps (for running outside Docker)
pip install -r requirements.core.txt
cd apps/target-fastapi && pip install -r requirements.txt && cd ../..
```

### Full Docker Demo (DGX Spark)
```bash
# Start all services
docker-compose up -d

# Wait for health checks (Triton takes ~30s)
docker-compose ps

# Run the demo flow
bash scripts/run_demo.sh

# Open dashboard
open http://localhost:8501
```

### Step-by-Step Manual Demo
```bash
# 1. Start target app (standalone, no Docker)
cd apps/target-fastapi
uvicorn main:app --port 8000 &

# 2. Verify normal request
curl http://localhost:8000/health

# 3. Seed exploit attack
bash scripts/seed_attack.sh

# 4. Open OpenClaw CLI (interactive)
python -m core.orchestrator.openclaw_cli interactive
# Then type: /simulate-alert
# Then type: /defend
# Then type: /status

# 5. Open dashboard
streamlit run dashboard/streamlit_app.py
```

### OpenClaw Commands Reference
| Command | Description |
|---------|-------------|
| `/defend` | Start defense cycle |
| `/replay` | Replay exploit against active version |
| `/status` | Show system status |
| `/benchmark` | Run burst benchmark |
| `/simulate-alert` | Inject mock IDS alert |

---

## Benchmark Evidence <a name="benchmark"></a>

Run the benchmark to produce hard numbers:

```bash
bash scripts/run_benchmark.sh
# Or: python -m core.orchestrator.openclaw_cli benchmark --burst-size 50 --with-defense
```

**Output files:**
- `artifacts/benchmark/benchmark_summary.json`
- `artifacts/benchmark/benchmark_summary.csv`
- Rich terminal table printed automatically

**Expected metrics on DGX Spark:**

| Metric | Expected (DGX Spark) | Notes |
|--------|----------------------|-------|
| Mutation candidates / cycle | 10 | Configurable 8â€“20 |
| Defense cycle latency | 5â€“15s | Depends on test suite size |
| Exploit replays / cycle | 10Ã—5 = 50 | 10 candidates Ã— 5 payloads |
| Triton inference latency | <50ms | Per model call |
| vLLM inference latency | <2s | Explanation generation |
| Burst throughput | 100+ rps | With cuDF analytics |
| Exploit success rate delta | >80% reduction | Vulnerable â†’ hardened |

---

## Screenshot Checklist <a name="screenshots"></a>

For live demo evidence, capture and place in `artifacts/`:

- [ ] `artifacts/gpu_screenshot.png` â€” DGX dashboard GPU utilization during benchmark
- [ ] `artifacts/dashboard_screenshot.png` â€” Streamlit dashboard with real data
- [ ] `artifacts/benchmark_terminal.png` â€” Rich terminal benchmark table
- [ ] `artifacts/openclaw_defend.png` â€” OpenClaw CLI defense cycle output
- [ ] `artifacts/exploit_before.png` â€” Exploit success before defense cycle
- [ ] `artifacts/exploit_after.png` â€” Exploit blocked after deployment

---

## Project Structure

```
/ZeroWall
â”œâ”€â”€ apps/target-fastapi/        # Vulnerable demo FastAPI app
â”‚   â”œâ”€â”€ main.py                 # 3 simulated vulnerable endpoints
â”‚   â”œâ”€â”€ test_app.py             # 25+ unit tests (verifier uses these)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agents/                 # 5 ZeroWall agents
â”‚   â”‚   â”œâ”€â”€ mutation_agent.py   # Generates 8â€“20 candidate plans
â”‚   â”‚   â”œâ”€â”€ exploit_agent.py    # Replays known attack payloads
â”‚   â”‚   â”œâ”€â”€ verifier_agent.py   # Runs pytest + bandit
â”‚   â”‚   â”œâ”€â”€ risk_agent.py       # Scores + recommends action
â”‚   â”‚   â””â”€â”€ explanation_agent.py # Judge-facing summaries
â”‚   â”œâ”€â”€ transforms/             # Safe deterministic AST transforms
â”‚   â”‚   â”œâ”€â”€ base.py             # Transform registry
â”‚   â”‚   â”œâ”€â”€ rename_identifiers.py
â”‚   â”‚   â”œâ”€â”€ reorder_blocks.py
â”‚   â”‚   â”œâ”€â”€ split_helpers.py
â”‚   â”‚   â”œâ”€â”€ swap_validators.py  # PRIMARY security hardening transform
â”‚   â”‚   â””â”€â”€ route_rotation.py
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ defense_loop.py     # Main multi-agent coordinator
â”‚   â”‚   â””â”€â”€ openclaw_cli.py     # OpenClaw command interface
â”‚   â”œâ”€â”€ deploy/
â”‚   â”‚   â””â”€â”€ controller.py       # Blue/green deploy + rollback
â”‚   â”œâ”€â”€ telemetry/
â”‚   â”‚   â”œâ”€â”€ collector.py        # Event collection â†’ JSONL
â”‚   â”‚   â””â”€â”€ rapids_analytics.py # cuDF GPU analytics
â”‚   â””â”€â”€ benchmark/
â”‚       â””â”€â”€ burst_sim.py        # Burst attack benchmark suite
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ triton-model-repo/      # Triton model repository
â”‚   â”‚   â”œâ”€â”€ mutation-planner/   # Transform type selector model
â”‚   â”‚   â””â”€â”€ risk-scorer/        # Candidate risk scoring model
â”‚   â””â”€â”€ clients/
â”‚       â”œâ”€â”€ triton_client.py    # Triton HTTP v2 client
â”‚       â””â”€â”€ vllm_client.py      # vLLM OpenAI-compatible client
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ streamlit_app.py        # Judge-facing metrics dashboard
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_demo.sh             # End-to-end demo flow
â”‚   â”œâ”€â”€ run_benchmark.sh        # Benchmark runner
â”‚   â””â”€â”€ seed_attack.sh          # Seed known exploit payloads
â”œâ”€â”€ docker-compose.yml          # All services: target, triton, vllm, core, dashboard
â”œâ”€â”€ Dockerfile.core             # ZeroWall core engine container
â”œâ”€â”€ requirements.core.txt
â””â”€â”€ .env.example
```

---

## Safety Disclaimer <a name="disclaimer"></a>

> âš ï¸ **HACKATHON SAFETY NOTICE**
>
> ZeroWall is built for safe, controlled demonstration purposes only.
>
> - The "vulnerable" endpoints do NOT expose real system resources, execute real commands, or perform any harmful operations
> - All simulated vulnerabilities are sandboxed in an in-memory dictionary
> - Exploit payloads target ONLY the local demo FastAPI container â€” no external network probing
> - No real offensive tooling is included in this project
> - The deploy controller only modifies the local demo app source file
> - All "exploits" are pre-defined, non-harmful HTTP requests that trigger simulated response patterns

This project demonstrates the *architecture* and *decision-making pipeline* of a moving target defense system. Real deployment would use actual vulnerability detection, but safety is the top priority for this demonstration.